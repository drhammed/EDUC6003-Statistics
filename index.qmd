---
title: "EDUC 6003 Advanced Statistics"
author: "Hammed Akande"
format: html
editor: visual
---

# Chapter 1- Reliability Index Analysis

## Introduction to Reliability Index Analysis

This tutorial will serve as a Guide to Enhanced Reliability Estimation with R

### What is Reliability?

Reliability refers to the consistency and precision of measurement instruments. In psychological testing, it assesses the extent to which a test produces stable and consistent results.

#### Importance of Reliability

*Measurement Precision*: Ensures that the scores accurately reflect the true attributes being measured.

*Research Validity*: High reliability is a prerequisite for valid conclusions and replicable research findings.

*Error Reduction*: Minimizes the impact of random errors, enhancing the clarity of observed relationships between constructs.

#### Types of Reliability

*Internal Consistency*: Degree to which items within a test measure the same construct.

*Test-Retest Reliability*: Consistency of scores over time.

*Inter-Rater Reliability*: Agreement between different raters or observers.

## Classical Test Theory (CTT) and Coefficient Alpha

Classical Test Theory Overview: CTT posits that each observed score is composed of a true score and an error score:

$$ X = T + E $$

where:

ùëã= Observed score ùëá= True score ùê∏= Error score

### Coefficient Alpha (Cronbach's Alpha)

This is a widely used measure of internal consistency reliability within CTT. It estimates the extent to which items on a test measure the same underlying construct.

$$
\alpha = \frac{N}{N-1} \left(1 - \frac{\sum \sigma^2_{E}}{\sigma^2_{X}}\right)
$$

Where:

$\alpha$ = Cronbach's Alpha

ùëÅ = Number of items in the test

$\sigma^2_{E}$ = Variance of the error scores

$\sigma^2_{X}$ = Variance of the observed total scores

#### Assumptions of Alpha

*Unidimensionality*: All items measure a single construct.

*Tau Equivalence*: Each item has the same true score variance.

*Independence of Errors*: Error terms are uncorrelated across items.

#### Limitations of Alpha

*Sensitivity to Tau Equivalence*: Violations can lead to underestimation or overestimation of reliability.

*Assumes Unidimensionality*: Not suitable for multidimensional scales without adjustments.

*Ignores Factor Structure*: Does not account for the underlying factor model of the test.

### Coefficient Omega

Coefficient Omega is a more robust alternative to Cronbach's Alpha, especially when the assumption of tau-equivalence (equal factor loadings) is violated. Omega takes into account the factor structure of the test, making it suitable for both unidimensional and multidimensional scales.

#### Types of Omega

*Omega Total (œâ‚Çú*): Accounts for all common factors, both general and specific.

*Omega Hierarchical (œâ‚Çï)*: Represents the proportion of variance attributable to a general factor alone.

Omega Total is given by:

$$
\omega_t = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{k} \lambda_i + \sum_{i=1}^{k} \theta_i}
$$

Where:

$\omega_t$ = Omega Total

$\lambda_i$ = Factor loading for item

$\theta_i$ = Unique variance (error variance) for item

$k$ = Total number of items

For a hierarchical model, Omega Hierarchical is:

$$
\omega_h = \frac{\lambda_g^2}{\lambda_g^2 + \sum_{i=1}^{k} \theta_i}
$$

where:

$\omega_h$ = Omega Hierarchical

$\lambda_g$ = Factor loading of the general factor

$\theta_i$ = Unique variance for item $i$

#### Advantages of Omega Over Alpha

*Factor Structure Incorporation*: Utilizes confirmatory factor analysis (CFA) to model the underlying structure.

*Less Sensitive to Tau Equivalence Violations*: Provides more accurate reliability estimates when tau equivalence is not met.

*Applicability to Multidimensional Scales*: Suitable for tests measuring multiple constructs.

## Calculations in R

Here, I will walk through the step-by-step process of calculating Cronbach's Alpha and Coefficient Omega using R.

Load the necessary packages.

```{r}

pacman::p_load(psych,lavaan,semTools,tidyverse,semTools,psych,tidyverse,lavaan,rio,parameters,
               nFactors,EGAnet,PCDimension)

```

## Loading the (BFI) dataset

The Big Five Inventory (BFI) is a widely used dataset in personality psychology, measuring the five major dimensions of personality. We'll focus on the Openness trait for this tutorial.

```{r}

# Make sure you've loaded the psych package


# Load the BFI dataset
data(bfi)

# load data file
bfi <- bfi
names(bfi)

# View the structure of the dataset
#str(bfi)




```

The dataset contains 2,800 participants with various demographic and personality-related variables. A1 to A5 correspond to items measuring the Openness trait.

### Selecting Openness Items

For this tutorial, we'll focus on five items measuring the Openness trait. These items are typically labeled as A1 to A5 in the BFI dataset.

#### a. Extracting Openness Items

```{r}

# Select columns that contain "O" in their names and remove rows with NA values
oppeness <- bfi %>%
  select(matches("^O")) %>%
  drop_na()

head(oppeness)

```

Each A variable represents a Likert-type item measuring Openness. Responses range from 1 to 6, treated as continuous variables for analysis.

### Handling Missing Data

Before we continue, let's check any missing data.

```{r}

# Check for missing values
sum(is.na(oppeness))


```

In this dataset, there are no missing values in the Openness items. If missing values were present, then we need to address that.

## Calculating Cronbach's Alpha

Recall that Cronbach's Alpha (Œ±) is a measure of internal consistency that measures how closely related a set of items are as a group.

### a. We can use the psych Package in R

```{r}

# Calculate Cronbach's Alpha using the psych package
#alpha_result <- psych::alpha(oppeness)

# Print the result
#print(alpha_result)



```

Because some items were negatively correlated with the first principal component, we probably should reverse them.

```{r}

# Reversing item O2 and O5

q <- c("O2","O5")
reverse <- function(x){
  x.reversed <- 7 +0 - x
}

oppeness[, c("O2R", "O5R")] <- reverse(oppeness[,c("O2","O5")])

# compare original and reversed responses
oppeness[1:6,]

```

Now, we can compute the alpha

```{r}

# compute alpha coefficient 

alpha_result <- psych::alpha(oppeness[, -c(2,5)])

# Print the result
print(alpha_result)


```

Explanation:

raw_alpha: The Cronbach's Alpha coefficient.

std.alpha: Standardized alpha, similar to raw_alpha.

G6(smc): Generalizability theory estimate with squared multiple correlations.

average_r: Average inter-item correlation.

S/N: Signal-to-noise ratio.

ase: Asymptotic standard error.

Confidence Intervals: Lower and upper bounds for alpha.

As we can see, the Cronbach's Alpha for the Openness items is 0.6, indicating strong internal consistency. The average inter-item correlation is 0.24, moderate and within psychological scales.

## Confirmatory Factor Analysis (CFA)

Confirmatory Factor Analysis (CFA) is used to verify the factor structure of a set of observed variables. In our case, we assume a unidimensional structure.

### a. Specifying the CFA Model

Let's specify a one-factor model where all items load on a single latent factor.

```{r}

# Specify model
mod1f <- "
openness =~ O1 + O2R + O3 + O4 + O5R
"

```

### b. Fitting the CFA Model

Fit the Model Using lavaan:

```{r}
#pacman::p_load(lavaan)
# Estimate model
fit.one.f <- cfa(mod1f, data=oppeness, std.lv=T, missing='direct', 
             estimator='MLR')

# The results can be viewed using the summary

summary(fit.one.f, fit.measures=T, standardized=T)



```

Explanation:

Estimator: Maximum Likelihood (ML).

Test Statistics: The Chi-square test is significant (ùëù\<0.001), indicating a poor fit.

Fit Indices:

CFI (0.94): above 0.90.

TLI (0.88): Below the threshold of 0.90.

RMSEA (0.068): Indicates relatively weak fit (‚â•0.05).

SRMR (0.029): Great fit (‚â§0.08).

```{r}

residuals(fit.one.f, type='cor')

```

The fit statistics from the Robust column in the summary suggest that this one-factor model provides a moderate fit to the data. While the Comparative Fit Index (CFI) of 0.94 indicates an acceptable fit, the Tucker-Lewis Index (TLI) of 0.88 is relatively lower, and the Root Mean Square Error of Approximation (RMSEA) is relatively high at 0.08.

The factor loadings (ùúÜÃÇ) show significant variation, ranging from 0.357 to 0.792, which implies that the assumption of equal loadings (tau equivalence) does not hold for this openness test. As a result, omega (œâu) should be a more suitable measure of reliability than Cronbach's alpha. We can compute the œâu estimate by running the `reliability` function from the semTools package on the one-factor model object (`fit1f`).

```{r}

reliability(fit.one.f)

```

`omega` and `omega2` measure reliability based on the variance expected by the model, while `omega3` measures it based on the actual variance seen in your data. The small difference between `omega` and `omega3` suggests that the model's predictions of variance are somewhat close to what‚Äôs observed in the sample.

## Calculating Categorical Omega

Recall that Coefficient Omega (œâ) is a reliability estimate that accounts for the factor structure of the test, providing a more accurate measure than Cronbach's Alpha, especially in the presence of multidimensionality or when tau-equivalence is violated.

Since factor loadings derived from polychoric correlations represent the link between the factor and the underlying latent response variables (not the actual responses from the items), using the œâu formula here would tell us how much of the variance in the latent response variables is explained by the factor. However, this doesn‚Äôt reflect the true variance in the sum of the observed item responses. Essentially, in this case, œâu would give us the reliability of a hypothetical total score based on the latent variables rather than the actual observed total score.

To correct for this, Green and Yang (2009b) proposed a new reliability estimate called œâu-cat, which adjusts the reliability calculation to align with the observed total score. This estimate is more accurate for unidimensional scales with categorical items, especially when the response patterns vary across items.

Since the psychoticism items use a 4-point rating scale, I fit the one-factor model using polychoric correlations and the WLSMV estimator (weighted least squares with mean and variance adjustment). This approach is recommended for confirmatory factor analysis involving polychoric correlations, as it is more robust than the maximum likelihood (ML) estimator.

```{r}
#load the data
potic <- import("data/potic.csv")

# Specify model
mod.one.fCat <- 'psyctcsm =~ DDP1 + DDP2 +
DDP3 + DDP4'


# Estimate model
fit.one.fCat <- cfa(mod.one.fCat, data=potic, std.lv=TRUE, ordered=T, estimator='WLSMV')

# Retrieving the results
summary(fit.one.fCat, fit.measures=TRUE, standardized=TRUE)


```

```{r}

# Estimate reliability

reliability(fit.one.fCat)


```

We can see that we have two estimates Cronbach‚Äôs alpha and differ a bit (0.80 & 0.77). The is because the 2nd alpha estimate is an example of ordinal alpha, which is calculated using a model based on polychoric correlations, while the first alpha was derived using the standard method based on product-moment covariances between the items. It's important to note that ordinal alpha measures the reliability of the sum of the continuous, latent-response variables rather than the observed categorical item responses. Additionally, it still assumes equal factor loadings (tau equivalence). Because of these limitations, I suggest disregarding the alpha value reported by semTools::reliability when the model uses polychoric correlations.

The values shown in the `omega` and `omega2` rows represent the œâu-cat estimate. The `omega3` row shows a variation where the denominator uses the observed sample variance of ( X ). So, we can see œâu-cat estimates suggest that 79% of the variance in the total score for the psychoticism scale is explained by the single psychoticism factor.

## Bifactor Models

A bifactor model is a useful way to represent a multidimensional structure. In this model, there‚Äôs a general factor that affects all items, while additional specific factors (or group factors) explain the relationships (covariation) between certain subsets of items beyond what the general factor accounts for.

For the model to work correctly, the general factor needs to be uncorrelated with the specific factors. In contrast to other Confirmatory Factor Analysis (CFA) models where all factors can correlate freely, allowing the general factor to correlate with a specific factor in a bifactor model can lead to issues like non-convergence or incorrect solutions.

### Omega Hierarchical (œâh)

When data fits well with a bifactor model, a reliability metric called **omega hierarchical (**$œâ_h$) is used. This measure reflects how much of the total score's variance is attributable to the single general factor, even though the data involves multiple dimensions.

Here, let's demonstrate the estimation of $œâ_h$ using R, I use data that from Flake, Ferland, & Flora, 2017 collected by administering the PCS to 154 students in an introductory statistics course.

```{r}
pcs <- import("data/pcs.csv")
names(pcs)

```

```{r}

modBf <- "
gen =~ TE1+TE2+TE3+TE4+TE5+OE1+OE2+OE3+OE4+LVA1+LVA2+LVA3+LVA4 +EM1+EM2+EM3+EM4+EM5+EM6
s1 =~ TE1 + TE2 + TE3 + TE4 + TE5
s2 =~ OE1 + OE2 + OE3 + OE4
s3 =~ LVA1 + LVA2 + LVA3 + LVA4
s4 =~ EM1 + EM2 + EM3 + EM4 + EM5 + EM6
"

#Specify model
fitBf <- cfa(modBf, data=pcs, std.lv=T, estimator='MLR', orthogonal=T)

#Retrieving the results
summary(fitBf, fit.measures=TRUE, standardized=T)

```

```{r}

fitmeasures(fitBf)

```

From the results above, we can the bifactor model fits the PCS data well, with robust model-fit statistics, CFI = 0.978, TLI = 0.972, RMSEA = 0.053. Thus, it is reasonable to calculate $œâ_h$ to estimate how reliably the PCS total score measures the general psychological-cost factor.

```{r}

reliability(fitBf)

```

The values shown under the "gen" column relate to the general psychological cost factor. The omega estimate (0.974) doesn‚Äôt take the specific factors into account when calculating the variance of the total score, so it‚Äôs not the appropriate reliability measure for the PCS total score. Instead, the **omega2** and **omega3** values under "gen" represent **omega hierarchical (**$œâ_h$). The difference between them is that **omega2** uses the model-implied variance of the total score, while **omega3** relies on the observed variance from the sample.

In simple terms, both $œâ_h$ values tell us that **91% of the variance in the PCS total score** is explained by the general psychological cost factor, after accounting for the specific factors that influence different content areas.

### Higher-Order Models

In the bifactor model example, I treated the multidimensional nature of the PCS items as a distraction when measuring a broad, general psychological cost construct. However, in other cases, researchers may propose a different structure where a broad overarching factor indirectly influences all the test items by acting through more specific, narrower constructs that directly affect different subsets of items. This kind of setup suggests a **higher-order model**, where a higher-order (or second-order) factor drives differences in several lower-order (first-order) factors, which, in turn, influence the individual item responses.

In this model, researchers assess how well the test captures both the overall score (reflecting the broad higher-order construct) and the subscale scores (reflecting the more specific lower-order constructs).

When the data fit a higher-order model, the reliability of the total score is measured by **omega-higher-order (**$œâ_{ho}$), which represents the proportion of the total score‚Äôs variance that can be attributed to the higher-order factor. The calculation of œâho uses parameter estimates from the higher-order model.

```{r}
#Specify the higher-order factor model for the PCS items

homod <- 'TE =~ TE1 + TE2 + TE3 + TE4 + TE5 
      OE =~ OE1 + OE2 + OE3 + OE4
      LV =~ LVA1 + LVA2 + LVA3 + LVA4
      EM =~ EM1 + EM2 + EM3 + EM4 + EM5 + EM6
      cost =~ TE + OE + LV + EM'
```

```{r}

#Estimate the model and get the results
fitHo <- cfa(homod, data=pcs, std.lv=T, estimator='MLM')
summary(fitHo, fit.measures=T)

```

#### Let's obtain omega-ho from a higher-order model for the Psychological Cost Scale

```{r}

reliabilityL2(fitHo, 'cost')

```

```{r}

#Obtain omega estimates for the subscale scores as measures of the lower-order factors
reliability(fitHo)
```

## Exploratory Omega Estimates

So far, the examples have relied on semTools‚Äô `reliability` (or `reliabilityL2`) function to calculate omega estimates based on CFA models. However, these omega estimates are only valid if the underlying model for the test is correctly specified. For instance, using $œâ_u$ as a reliability estimate wouldn‚Äôt be appropriate if the true model is multidimensional, as indicated by a poor fit for a single-factor model.

When a hypothesized CFA model doesn‚Äôt adequately fit the data‚Äîwhich is common in the early stages of test development‚ÄîExploratory Factor Analysis (EFA) can help identify the test‚Äôs dimensional structure. After determining the optimal number of factors for a test, you can use the `omega` function from the **psych** package (Revelle, 2020) to estimate omega based on the EFA parameters. This estimate will reflect the proportion of total score variance attributable to a general factor that influences all items.

```{r}

#Determine number of factors

n_factors(pcs, package = "all")

```

```{r}

#Determine omega for 4 factor model
omega(pcs, nfactors = 4, plot = F)

```

The results from the exploratory factor analysis indicate that the four-factor model provides a strong fit, with an overall omega total of 0.98, suggesting excellent reliability for the total score. The omega hierarchical value of 0.85 indicates that 85% of the total-score variance is attributable to a general factor, showing that the general psychological cost factor plays a dominant role in explaining the variance across items. Additionally, the individual subscales also show good reliability, with their omega total values ranging from 0.90 to 0.94.

# Chapter 2- Simple Linear Regression (SLR)

Linear model and Linear regression are technically synonyms and we often use either terms when quantifying the effect of a "**continuous"** independent variable on a"**continuous"** dependent variable. The difference between this and ANOVA is that **ANOVA** is usually used when quantifying the effect of a "**discrete (or categorical)"** independent variable on a"**continuous"** dependent variable. So, it is important to note that- *ANOVA is also a linear regression!* In fact, if you run "anova" function on linear model object, you'll most likely get the same p-value.

**Regression** generally refers to the fact that we are quantifying the relationship between a response variable and (one or more) predictor variables. In the case of **SLR**, both the response and the predictor are *numeric* variables and we are using a single predictor (independent) variable. Later we will use multiple predictor variables (multiple regression). Also, this models tells us that our model for Y is a linear combination of the predictors X. (In this case just one predictor)! For now, this always results in a model that is a line, but this is not always the case (and we may see this later on in the tutorial).

Like ANOVA, in SLR, we often talk about the assumptions that this model makes. This include-

1.  Linearity- the relationship between Y and x is linear, of the form $\beta_0 + \beta_1x$.
2.  Independent. The errors $\epsilon$ are independent.
3.  Normality. The errors, $\epsilon$ are normally distributed. I.e. the "error" around the line follows a normal distribution.
4.  Equality of Variance. At each value of x, the variance of Y is the same.

To learn extensively about the model assumptions, you may want to check this invited lecture I gave last Winter. [Testing Model Assumptions](https://drhammed.github.io/Biol322_2024_lecture/#2).

For this lab, we will be using a year dataset on Corvettes sales in Virginia Beach, Virginia. Using this data, ten Corvettes (between 1 and 6yrs old) were randomly selected and the below data shows the sales price (in hundreds of dollars) denoted by y and the age (in years) denoted by x.

![](images/Corvettes-01.PNG){fig-align="center"}

#### Tasks

1.  Graph the data in a scatterplot to determine if there is a possible linear relationship.
2.  Compute and interpret the linear correlation coefficient, r.
3.  Determine the regression equation for the data.
4.  Graph the regression equation and the data points.
5.  Identify outliers and potential influential observations.
6.  Compute and interpret the coefficient of determination, r2.
7.  Obtain the residuals and create a residual plot. Decide whether it is reasonable to consider that the assumptions for regression analysis are met by the variables in questions.
8.  Using 5% significance level, can we say the data provide sufficient evidence to conclude that the slope of the population regression line is not 0 and, hence, that age is useful as a predictor of sales price for Corvettes?
9.  Obtain and interpret a 95% confidence interval for the slope, Œ≤, of the population regression line that relates age to sales price for Corvettes.
10. Obtain a point estimate for the mean sales price of all 4-year-old Corvettes.
11. Determine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.
12. Find the predicted sales price of Jack Smith's 4-year-old Corvette.
13. Determine a 95% prediction interval for the sales price of Jack Smith's 4-year-old Corvette.

This is the [link](https://drive.google.com/drive/folders/1nb1zgxr_IIV9271-BWmffmMSukOYf3ja?usp=sharing) to the main google drive folder for all the data to complete this tutorial!

To start, we need to load in the packages and data

```{r}

pacman::p_load(ggplot2,dplyr,ggplot2,ggpubr,lmtest,data.table,car)

# Load data
df <- read.csv("data/corvettes.csv")

# Rename columns ( x = Age and y = Price)
df <- df %>%
  rename(Age = x, Price = y)

#check the first few rows
head(df)


```

**1: Graph the Data in a Scatterplot** Let's create a scatterplot to visualize the relationship between Age and SalesPrice. Also, we can rename the x and y to be Age and Price respectively.

```{r}

# Scatterplot of Age vs. SalesPrice

ggplot(df, aes(x = Age, y = Price)) +
  geom_point() +
  labs(title = "Scatterplot of Age vs. Sales Price", x = "Age", y = "Price") +
  theme_minimal()


```

Interpretation- The points seem to follow a linear pattern, although with a negative relationship.

**2: Compute and Interpret the Linear Correlation Coefficient**

The Pearson Correlation is a statistical method that calculates the strength and direction of linear relationships between continuous variables. It produces a sample correlation coefficient, r, which can be used to evaluate whether there is a linear relationship among the same variables in the population. This population correlation coefficient is represented by œÅ ("rho") and is a parametric measure.

Pearson correlation indicates:

-   Whether there is a statistically significant linear relationship between two continuous variables

-   It also shows the strength of this linear relationship (i.e., how close the relationship is to being a perfectly straight line)

-   Finally, it reveals the direction of this linear relationship (increasing or decreasing)

It is however important to note that Pearson Correlation cannot address non-linear relationships or relationships among categorical variables. To address relationships that involve categorical variables and/or non-linear relationships, you need to consider the equivalent non-parametric test (e.g., Spearman's rank correlation).

Also, while Pearson Correlation reveals *associations* among (continuous) variables, you should remember that "*Correlation* *does not imply causation,*" no matter how large the correlation coefficient is.

```{r}

# Compute correlation
df_cor <- cor(df$Age, df$Price)
df_cor

```

**Interpretation**

The correlation coefficient is **-0.968**. This r-value indicates a robust negative linear correlation, given its proximity to -1 and negative sign. This strong negative linear correlation suggests that data points should closely cluster around a downward-sloping regression line, (*which aligns with the graph above*). Consequently, the presence of a strong negative linear relationship between Age and Price supports the continuation of linear regression analysis.

**3. Regression equation for the data.**

```{r}

# Fit linear model
mod <- lm(Price ~ Age, data = df)

# model summary
summary(mod)


```

From the output, we can interpret the regression equation:

Price = $\beta_0$ $+$ $\beta_1 .$ Age

where $\beta_0$‚Äã is the intercept and $\beta_1$‚Äã is the slope.

So, from the result above, the regression equation is: Price = 29160.2 - (2790.3)(Age). So what if a newly sold Corvettes was 10years old? What would be the price? *Y* = 29160.2 - (2790.3)(10) which equals \$1257.2

**4: Graph the Regression Equation and the Data Points**

Add the regression line to our scatterplot.

```{r}

# Scatterplot with regression line & equation

ggplot(df, aes(x = Age, y = Price)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "blue") +
  stat_regline_equation(
    aes(label = ..eq.label..),
    label.x = 3, label.y = max(df$Price) - 1000, 
    coef.digits = 2
  ) +
  # Add R^2
  stat_regline_equation(
    aes(label = ..rr.label..),
    label.x = 3, label.y = max(df$Price), 
    rr.digits = 2
  ) +
  labs(title = "Scatterplot with Regression Line", x = "Age", y = "Price") +
  theme_minimal()


```

**5: Identify Outliers**

From the plot, there seem to be no points that lie far from the cluster of data points or far from the regression line; thus, no possible outliers.

**6. Compute and interpret the coefficient of determination,** $r^2$.

```{r}

# r-squared value
rsquared <- summary(mod)$r.squared
rsquared


```

The $r^2$ = 0.937; therefore, about 93.7% of the variation in the price data is explained by age. I.e., as the car gets older, the value/price drops! The regression equation appears to be very useful for making predictions since the value of $r^2$ is close to 1.

Note- we also have adjusted R-square. R-square technically measures the variation of a regression model (variation in Y given x). R-squared either increases or remains the same when new predictors are added to the model. Adjusted R-squared measures the variation for a multiple regression model, and helps you determine goodness of fit. For the purpose of this SLR (one predictor, we are not adding more), so we can decide to intepret any of them. But if you have multiple predictors, you may want to look into Adj. $r^2$)!

**7. Residuals**

Let's plot residuals to check for patterns, which can indicate whether regression assumptions are met.

```{r}

# Get residuals
df$residuals <- resid(mod)

# Residual plot
ggplot(df, aes(x = Age, y = residuals)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Residual Plot", x = "Age", y = "Residuals") +
  theme_minimal()


```

We can also check the assumption of constant variance using the Breusch-Pagan Test.

The $H_0$ and $H_A$ can be considered to be:

$H_0$ : Homoscedasticity. The errors have constant variance about the true model.

$H_a$ : Heteroscedasticity. The errors have non-constant variance about the true model.

```{r}

bptest(mod)

```

**Interpretation**- The residual plot shows a random scatter of the points. Here, we can see that at any fitted value, the mean of the residuals is roughly 0. As such, the linearity assumption holds true.In fact, that is why we generally add a horizontal line at $y = 0$ to emphasize this point.

Also, from the bpTest, we see a large p-value (0.13), so we do not reject the null hypothesis of homoscedasticity, and thus conclude constant variance assumption about the true model.

**Also, to assess the normality of the residuals**

```{r}

# 2. Q-Q Plot of Residuals
ggplot(df, aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals", x = "Theoretical Quantiles", y = "Sample Quantiles") +
  theme_minimal()

```

We can also use the shapiro test to confirm the normality

```{r}

shapiro.test(resid(mod))

```

$H_0$: The residuals are normally distributed.

$H_a$: The residuals are not normally distributed.

If the p-value is low (\< 0.05), it suggests evidence against the $H_0$, indicating that the residuals may not follow a normal distribution. However, if the p-value is high (\> 0.05), it supports the $H_0$, suggesting that the residuals are approximately normally distributed.

In short, we can say if the points of the plot do not closely follow a straight line, this would suggest that the data do not come from a normal distribution.

**8. Using 5% significance level, can we say the data provide sufficient evidence to conclude that the slope of the population regression line is not 0 and, hence, that age is useful as a predictor of sales price for Corvettes?**

Here, we need to state the hypothesis. The hypothesis from the question above is that-

$H_0$: $\beta = 0$ (Age is not a useful predictor of price.)

$H_a$: $\beta \neq 0$ (Age is a useful predictor of price.)

Now that we've done that, remember our alpha = 0.05 and our critical value or rejection region is that we should reject the null hypothesis if alpha is less than 0.05. Finally, to answer the question, check the regression output and see the "coefficient". From the result of the regression, you can see that p-value \<0.001

```{r}
# Get p-value for slope
p_value <- summary(mod)$coefficients[2, 4]
p_value

```

**Interpretation**- Since the P-value \< 0.05, we have evidence to reject the null hypothesis. In other words, we have enough evidence to conclude that the slope of the population regression line is not zero. In other words, age is a useful predictor of price for Corvettes.

**9. Obtain and interpret a 95% confidence interval for the slope, Œ≤ , of the population regression line that relates age to sales price for Corvettes.**

```{r}

# 95% confidence interval for the slope
confint(mod, "Age", level = 0.95)


```

Look at the result above, We are 95% confident that the slope of the true regression line is somewhere between -3381.295 and -2199.288. In other words, we are 95% confident that for every year older Corvettes get, their average price decreases somewhere between \$3,381.295 and \$2,199.288.

**10. Obtain a point estimate for the mean sales price of all 4-year-old Corvettes.**

We can use the model to predict the mean price for a Corvette of age 4.

```{r}

# Predict mean sales price for age 4
predict(mod, newdata = data.frame(Age = 4))


```

Look at the result, the point estimate (PRE_1) is \$17,999.03.

**11. Determine a 95% confidence interval for the mean sales price of all 4-year-old Corvettes.**

```{r}

# 95% confidence interval for mean price of 4-year-old Corvettes
predict(mod, newdata = data.frame(Age = 4), interval = "confidence", level = 0.95)


```

From the result, we are 95% confident that the mean sales price of all four-year-old Corvettes is somewhere between \$16,958.46 and \$19,039.6.

**12. Find the predicted sales price of Jack Smith's selected 4-year-old Corvette.**

```{r}
# Predicted sales price for a specific 4-year-old Corvette
predict(mod, newdata = data.frame(Age = 4))


```

The predicted sales price is \$17,999.03.

**13. Determine a 95% prediction interval for the sales price of Jack Smith's 4-year-old Corvette.**

```{r}
# 95% prediction interval for Jack's 4-year-old Corvette
predict(mod, newdata = data.frame(Age = 4), interval = "prediction", level = 0.95)

```

We are 95% certain that the individual sales price of Jack Smith ºs Corvette will be somewhere between \$14,552.92 and \$21,445.14.

# Chapter 3- Multiple Linear Regression

So far, we have learned how to calculate a linear regression equation and make predictions. However, what if there are other potential independent variables to consider? In fact, It is uncommon for a dataset or research study to have only one predictor. Similarly, it is rare for a response variable to depend solely on a single variable. In this chapter, we will expand our simple linear regression (SLR) model to include multiple predictors. Multiple regression allows us to incorporate multiple independent variables and assign weights to each of them, resulting in more accurate predictions. The process of conducting a multiple regression in R is similar to that of linear regression, with the difference being the inclusion of additional independent variables.

For this lab, we shall be using the "autompg" dataset containing car information. This dataset provides data on fuel economy from 1999 and 2008 for about 38 popular models of cars. It contains a response variable known as "mpg," which records the city fuel efficiency of cars, alongside several predictor variables detailing the vehicle attributes. You can download the data from our google drive folder. See the link [here](https://drive.google.com/file/d/1Hk_w8GMsFSajXT4sj57zhzax1oCm1zew/view?usp=sharing). For anyone using R, you can find the dataset loaded with the *ggplot2* package.

*Brief description of the variables in the data*

| Variable | Type    | Description                   |
|:---------|:--------|:------------------------------|
| mpg      | numeric | city fuel efficiency          |
| cyl      | integer | number of cylinders           |
| displ    | numeric | engine displacement in liters |
| hp       | numeric | horse power                   |
| wt       | numeric | Weight                        |
| acc      | numeric | acceleration                  |
| year     | integer | year of manufacturing         |

At this point, our focus will be on utilizing two variables, "wt" and "year," as predictor variables. In other words, we aim to create a model that predicts a car's fuel efficiency (mpg) based on its weight (wt) and the model year (year). To achieve this, we will formulate the following linear model:

$$ Y_i = \beta_i + \beta_1X_1 + \beta_2X_2 + \epsilon_i, \hspace4ex i = 1, 2, ‚Ä¶,n$$

where $\epsilon_i \sim N(0,\alpha^2)$ , $x_{i1}$ = the weight (wt) of the $i$ car, and $x_{i2}$ as the model year (year) of the $i$ car.


**Load the data**

```{r}

auto <- fread("data/autompg.csv")

```


**Task- before we do for multiple predictors, let's quickly revist SLR and use one predictor. For this part, perform a simple linear regression of mpg against wt. What's the R-squared? is weight a good predictor of mpg? (refer back to the tutorial on SLR- we just covered all of that)!**


```{r}
#| echo: false

# Fit SLR
#slr_model <- lm(mpg ~ wt, data = auto)

#summary(slr_model)


```


Your model summary should look like below.

![](images/model_summary_mpg_vs_wt-01.PNG){fig-align="center"}


*Okay, now that we have done that- we can proceed to multiple linear regression. Recall, we want to build a model with mpg as dependent variable, while wt and year as independent variables.*



## Build Multiple Regression in R

Next, we‚Äôll conduct a multiple linear regression using mpg as the dependent variable and both wt and year as predictors.

```{r}

# Fit the multiple linear regression model
mlr_model <- lm(mpg ~ wt + year, data = auto)

summary(mlr_model)


```




**Questions**

1) Looking at the result, you should see tables that are similar to the ones in our previous example (our model summary is new). Take a look at the "model summary" to determine the new R squared and Standard Error of the Estimate. Compare that with the previous regression output- ***has the inclusion of additional variables (year) resulted in an improvement in our model***?


Note- *the new R squared has improved, and the Standard Error has reduced! This indicates that our multiple regression model is more precise than the previous linear regression model.*



2) ***Do all our variables hold statistical significance***?

You can verify this by checking the table. The Pr(> t|) indicate whether our independent variables have a statistically significant association with our dependent variable. It also helps us determine whether our model is a good fit for explaining the variation in the mpg.

In our case, we have evidence to believe it is, given that the significance  is way below 0.05 (<0.001).

Lastly, check the "estimate" column. Here we find the value for each of our independent variables (wt and year) and also the intercept.


So, how can we write our multiple regression equation? mpg (*Y*) = -14.64 -- 0.0066(weight) + 0.761(year). Thus, if we added a new car and had some basic data like the year and weight, we would be able to estimate, with a relatively high degree of confidence, how many mpg would be used given the predictors.



**Interpretation**

Here, the constant = -14.64 represents our estimate for the intercept, i.e.- the mean miles per gallon for a car that has a weight of 0 pounds and was manufactured in 1900 (the start year of our dataset). As we can see here that the estimate is negative, which, in the real world, is physically impossible. However, this is not surprising because we cannot realistically expect our model to accurately predict the fuel efficiency of cars from 1900 that weigh 0 pounds because such vehicles never existed anyways! So, like simple linear regression, this value (intercept) represent the mean of Y when all predictors are set to 0.

However, the interpretation of the coefficients of our predictors is slightly different from previous SLR. For instance, the estimate of -0.0066 for "wt" = the expected average change in miles per gallon for a one-unit increase in weight for cars of a specific model year, with the year being held constant. Note that this estimate is negative, which aligns with our expectations, as, in general, fuel efficiency tends to decrease for larger vehicles. However, in the context of multiple linear regression, this interpretation is contingent upon a fixed value for another predictor, such as "year" in our case. This means that the relationship between fuel efficiency and weight might not hold true when additional factors, like the model year, are taken into account, potentially causing a reversal in the sign of our coefficient.

Lastly, the estimate of 0.761 for "year" = the expected average change in miles per gallon for a one-year increase in the model year for cars with a specific weight, where weight is held constant now. It is not far from expectation that this estimate is positive since one would anticipate that, over time, as technology advances, cars with the specific weight would achieve better fuel efficiency compared to their earlier counterparts.

Note- Sometimes, you may discover that the model is not statistically significant, or that one independent variable does not hold statistical significance. In such instances, you may want to rerun the model, eliminating insignificant or redundant variables. Ideally, it is good to do some variable importance selection on your predictors before including them in the model (or use some prior knowledge of the system). It may take several attempts to run multiple regression models to find the best-fitting model for the data. Generally, it is good to have model with a low standard error of the estimate, high R squared and relatively simple. A model with three independent variables, a relatively high R squared and low standard error may be preferable to a model with 19 independent variables and a high R squared and low standard error (this is why variable importance is crucial)!

To do variable importance selection, you can calculate the multicollinearity. In R, you can compute the Variance Inflation Factor (VIF).

```{r}

# Calculate VIF for the multiple linear regression model

vif_mod <- vif(mlr_model)
vif_mod


```



The VIF values indicate the degree of multicollinearity for each variable in the model. In our case, the VIF (for both wt and year) is around 1.104. Ideally, a VIF of 1 means that variables are not correlated and no multicollinearity in the regression model. Generally, a VIF >6 is considered a sign of high multicollinearity between the predictor variables and can affect the stability and interpretability of your regression model. You may need to address multicollinearity by either removing one of the correlated variables (redundant) or you can use dimensionality reduction techniques like Principal Component Analysis (PCA) to reduce dimensions.




# Chapter 4- Least Square Approach

So far, we've talked about simple linear regression and its assumption and that simple linear regression models the dependent variable Y as a **linear** function of independent variable (or variables in the case of multiple predictors) X. This means we would expect a plot similar to the one below (you can use this [dataset](https://drive.google.com/file/d/18BoUebca3hSG2nn_T545hSbxY526zIMg/view?usp=sharing) to reproduce the plot (*make a scatter plot of weight vs height*).

```{r}
#| echo: false

#load the data
st_data <- fread("data/st_data.csv")



# Fit a linear model to get the R-squared value
model <- lm(wt ~ ht, data = st_data)
r_squared <- summary(model)$r.squared

# Extract the intercept and slope
intercept <- coef(model)[1]
slope <- coef(model)[2]

# Format the regression equation as a string
equation <- paste0("y = ", round(intercept, 2), " + ", round(slope, 2), " * x")

# Create the scatter plot with R-squared annotation, regression line, and equation
ggplot(st_data, aes(x = ht, y = wt)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "red") +  # Add regression line
  labs(title = "Scatter Plot of Weight vs Height", x = "Height", y = "Weight") +
  theme_minimal() +
  annotate("text", x = min(st_data$ht), y = max(st_data$wt), 
           label = paste("R¬≤ =", round(r_squared, 3)), hjust = 0, vjust = 1, size = 5, 
           color = "blue") +
  annotate("text", x = min(st_data$ht), y = max(st_data$wt) - 5, 
           label = equation, hjust = 0, vjust = 1, size = 5, color = "darkgreen")



```



Looking at the plot, do you think this line best summarizes the trend between the students height and weight?

Before we proceed, let's introduce equation of the best fitting line.

$$\hat{y} = \beta_0 + \beta_1 x_1$$

where:

-   $\hat{y}_i$ is the predicted response (or fitted value) for experimental unit *i*

-   $y_i$ denotes the observed response for experimental unit *i*

-   $x_i$ denotes the predictor value for experimental unit *i*


Now, let's apply this formula on the plot above. Remember from the plot above, our regression equation is $\hat{y}$ = -266.53 + 6.14x or $\hat{weight}$ = -266.53 + 6.14 height

The first data on the plot shows that student 1 has a height of 63 inches and a weight of around 127 pounds (i.e. $x_1$ = 63 and $y_1$ = 127). Assuming we know this student's height but not weight, we could use the equation of the line to predict the student's weight. Thus, we'd predict the student's weight to be -266.53 + 6.14(63) or 120.29 pounds. Which means our predicted y ($\hat{y}$) = 120.29 pounds. Apparently, this is not the same as can be seen on the plot ($y_1$ = 127) and this means we have **prediction** or **residual error**. As a matter of fact, the residual error can be calculated as 127-120.29= 6.71 pounds. You can do the same for all values of x (heights) and get your observed (y) responses, your predicted responses and residual errors. In summary, when we use the equation $\hat{y} = \beta_0 + \beta_1 x_1$ to make prediction of the actual response of $y_i$, we make prediction or residual error $e_i = y_i - \hat{y}$ (which means that the size of the residual error depends on the data point).



An important question, however, is "What is the best fitting line or how do we define a good line"? There are several lines we could use, and our aim is to identify one that is characterized by "minimal errors." In other words, a line that fits the data "**best**" will be one where the ***n*** **prediction errors, ---** each corresponding to an observed data point **--- are minimized overall**. The next question is then, how do we identify such a line? There are numerous methods we could employ for this purpose. One way to achieve this is by applying the "**least squares approach**" which means we need to find the line that "minimizes the sum of squared prediction errors. In other words, we need to find the values of $b_0$ and $b_1$ that can make the sum of the squared residual errors as minimal as possible.

Mathematically, we need to find the values of $b_0$ and $b_1$ that minimizes

$$\text Q = \sum^{n}_{i==1} (y_i - \hat{y})^2$$


Where our

-   prediction (residual) error for data point *i* = $y_i - \hat{y}_i$

-   the squared prediction (residual) error for data point *i* = $(y_i - \hat{y}_i)^2$

-   Lastly, the summation symbol indicate that we should add up the squared prediction (residual) errors for all *n* data points.

So, to summarize, imagine you fit two lines to the data and you want to know which one best describe the trend, you'd pick the line with the lowest sum of squared residual (prediction) error as the **best line**.

From the plot above, note that that if we (manually) adopt the least square method described above to find the equation of the line that minimizes the sum of squared residual error, we may encounter big issues. Specifically, we would need to execute this procedure for an infinite variety of potential lines (not convenient!). Luckily, someone has already done the laborious work of deducing formulas for both the intercept and slope in the equation of the line that minimizes the sum of squared residual error (derived using Calculus).

Here, we minimize the equation for the sum of the squared residual errors:

![](images/LS_formula1.PNG){fig-align="center"}

thus, we can get our least squares estiates for $b_0$ and $b_1$ by taking the derivative with respect to $b_0$ and $b_1$, set to 0, and solve for $b_0$ and $b_1$:

![](images/LS_formula2-01.PNG)

and:

![](images/b1_form-02.PNG){fig-align="center"}



Given that the formulas for $b_0$ and $b_1$ are derived using the least square approach, you may see the regression equation being referred to as the **least squares line, least squares regression line or the estimated regression equation**. However, it is important to note that in this approach, we've made no distributional assumption about the data. We assumed they're fixed and not random and follow a linear trend.

Ideally, you won't have to worry about the formulas for $b_0$ and $b_1$ and this is because, there are so many statistical software that can find the least squares for you. However, a question that often pop-up is that- why do we try to minimize the sum of *squared* errors rather than just the errors themselves? The reason is that if we didn't square the residual error above, the positive and negative residual errors would cancel each other out when we sum them, consequently yielding 0. This approach is referred to as *least squares* because it aims to *minimize* the sum of *squared* errors, thereby seeking to determine *the least squares.*


### Example of Least Square Approach using data

For this, we would use the "car" dataset. This is an open dataset available in R software (and online). Very briefly, it shows the relationship between the speed and stopping distance of cars. You can downoad it from our google folder [here](https://drive.google.com/file/d/1_VhFQlnLENbyYVXNWlDGYDc6x5QdMznX/view?usp=sharing). load this dataset to your R.

Traditionally, we would now compute $\hat{b_1}$ and $\hat{b_0}$ for the cars dataset. However, like I explained above, we can easily allow R or other software to do this for us.

From the data, note that our x = speed and y = dist. From your data, we need to calculate the three sums of squares defined above. For simplicity, we will regard to the summation sign above as "S".

So, compute Sxy, Sxx, Syy

Sxy = sum((x - mean(x)) * (y - mean(y)))
Sxx = sum((x - mean(x)) ^ 2)
Syy = sum((y - mean(y)) ^ 2)
c(Sxy, Sxx, Syy)


```{r}


# Load the built-in cars dataset in R
data("cars")

# Define x and y from the car dataset
x <- cars$speed
y <- cars$dist

# get the means of x and y
mean_x <- mean(x)
mean_y <- mean(y)

# Compute Sxy, Sxx, Syy
Sxy <- sum((x - mean_x) * (y - mean_y))
Sxx <- sum((x - mean_x)^2)
Syy <- sum((y - mean_y)^2)

# Display the results
c(Sxy = Sxy, Sxx = Sxx, Syy = Syy)


```




Finally, we need to calculate the $\hat{b_0}$ and $\hat{b_1}$

$\hat{\beta_1}$ = Sxy / Sxx

$\hat{\beta_0}$ = mean(y) - $\hat{\beta_1}$ \* mean(x)



```{r}

# Calculate beta_1 (slope)
beta_1 <- Sxy / Sxx

# Calculate beta_0 (intercept)
beta_0 <- mean_y - beta_1 * mean_x

# print results
c(beta_0 = beta_0, beta_1 = beta_1)

```

#### **Interpretation**

**What do these values tell us about our dataset?**

Let's use the regression line formula again, $\hat{y} = \beta_0 + \beta_1 x_1$

or more formal way,

$\hat{y}_{i, dist} = \beta_0 + \beta_1 {x_1,speed}$

Remember $\hat{\beta_0}$ is the constant (intercept) and $\hat{\beta_1}$ is the slope.

From the result, the slope parameter $\hat{\beta_1}$ tells us that for every unit increase in speed of one mile per hour, the **mean** stopping distance increases by $\hat{\beta_1}$. It is important to note that we're talking about the mean estimate- If you remember, from the formula of equation of best fit line $\beta_0 + \beta_1 x_1$ represent the mean of y (here, distance) for a particular value of $x$ (speed). So, the slope ($\hat{\beta_1}$) tells us how the mean of distance is affected by speed.

Specifically, the $\hat{\beta_1}$ = 3.93 implies that for every unit increase in speed (of one mile per hour), the **estimated** *mean* stopping distance increases by 3.93ft. Also note the word "estimated" since $\hat{y}$ is the estimated mean of the observed response Y, so $\hat{\beta_1}$ tells us how the estimated mean of Y is affected by changing $x$.

**What does** $b_0$ **tell us?**

In a very brief term, the intercept (or $\hat{\beta_0}$) represent the value of Y when all the predictors = 0 (i.e.- **mean** stopping distance for a car traveling zero miles per hour or not moving at all).

Here, the $\hat{\beta_0}$ tells us that a vechicle travelling at zero mile per hour is predicted to have -17.58 stopping distance. In other words, the estimated mean stopping distance for a car not moving is‚àí17.58 ft. Obviously this doesn't make sense, because does that mean when you apply the brakes to a car that is not moving, it moves backwards? Anyways this is not surprising becuase we "extrapolated" beyond the range of the x values (model scope). It doesn't make sense to say you're travelling at speed of zero miles per hour (so intercept here doesn't make much sense)! More information on extrapolation can be found in this [blog](https://blog.minitab.com/en/adventures-in-statistics-2/regression-analysis-how-to-interpret-the-constant-y-intercept) and we will see more of it shortly below in the tutorial.


### Predictions

Let's rewrite the estimated regression line

$\hat{y}_{i, dist} = \beta_0 + \beta_1 {x_1, speed}$

$\hat{y}_i = -17.58 + 3.93x$

or

$\hat{dist}_i = -17.58 + 3.93speed$

We can now use this equation to make predictions. If you check the data, the speed ranges from 4 to 25.

**Question 1**- Can we make a prediction for the stopping distance of a car traveling at 9 miles per hour?.

***Hint- It's easy, slot in 9 in the equation above (***$-17.58 + 3.93speed$***). Your answer should be ~17.79 (***This tells us that the estimated mean stopping distance of a car traveling at 9 miles per hour is 17.79***).***

**In the same way, we can make predictions for unknown data or unseen time point.** For example,

**Question 2-** make a prediction for the stopping distance of a car traveling at 6 miles per hour. This is referred to as **interpolation** as 6 is not an observed value of speed (But it is in the data range.)- you can check the speed and you'd agree that 6 is not there. Your answer should be **6**

Finally, we can make a prediction for the stopping distance of a car that is outside of the data range (**extrapolation!**)

**Question 3-** make a prediction for the stopping distance of a car traveling at 100 miles per hour. This is extrapolation as 100 is not within the range of speed and not an observed value. So, we are only transferring our model to that time point (learning from the model- Machine Learning)! ***Your answer should be = 375.42ft***

Although cars can travel 100 miles per hour today (although with fines from police), but maybe not many years ago! This is quite similar to the similar issue when interpreting $\hat{b_0} = -17.58$ (that is estimated mean stopping distance when speed = 0). This implies that we should be less confident in the estimated linear relationship outside our data range.




### Residuals

Recall our residual formula, $e_i = y_i - \hat{y_i}$

We can calculate the residual for the prediction we made for a car traveling 9 miles per hour. First, we need to obtain the observed value of distance for this speed value (10). You can look at the data table, what's the value of distance when speed = 9?

Answer, dist == 10.

Then, we calculate our $e = 10 - 17.81$ = -7.81.

The negative residual value indicates that the observed stopping distance is actually 7.81 feet less than what was predicted.




### Estimating the Variance

We can now use the residuals for each data point to compute the variance

In regression case, for each $y_i$, we can use a different estimate of the mean, that is $\hat{y_i}$ to calculate the variancee.

```{r}

#Calculate Predicted Values and Residuals
# Predicted values (y-hat) based on the linear model
y_hat <- beta_0 + beta_1 * x

# Residuals (y - y-hat)
residuals <- y - y_hat



# Now, we can calculate the residual variance
variance <- sum(residuals^2) / (length(y) - 2)
variance


```



Similar to the univariate measure of variance, variance = 236.53 lacks a meaningful practical interpretation in this context of stopping distance. However, by taking the square root, we can obtain the standard deviation of the residuals, often referred to as the *residual standard error.*

**Question- take the square root to compute the residual standard error**

Your residual standard error should be ~ **15.38.**

**Interpretation**- This indicates that our average estimates of stopping distance are generally inaccurate by approximately 15.38 feet.




### Variation Decomposition

Here, we will briefly define 3 of the metrics used for decomposition of variation.

#### 1. Sum of Squares Total

The term "Sum of Squares Total," denoted as SST, represent the total variation present in the observed y values.

![](images/SST.PNG){fig-align="center"}

#### 2. Sum of Squares Regression

The term "Sum of Squares Regression," often abbreviated as SSReg, denotes the portion of variation in the observed y values that can be accounted for or explained by the regression.

![](images/SSReg.PNG){fig-align="center"}

#### 3. Sum of Squares Error

The term "Sum of Squares Error," (SSE), denotes the portion of variation in the observed y values that remains unexplained or unaccounted for. You may frequently see SSE written as RSS, which stands for "Residual Sum of Squares."

![](images/SSE.PNG){fig-align="center"}

Now, you can use the formula of each of them to calculate their values in R.

-   SST = sum((y - mean(y)) \^ 2)

-   SSReg = sum((y_hat - mean(y)) \^ 2)

-   SSE = sum((y - y_hat) \^ 2)


```{r}

# Total Sum of Squares (SST)
SST <- sum((y - mean(y))^2)

# Regression Sum of Squares (SSReg)
SSReg <- sum((y_hat - mean(y))^2)

# Sum of Squares Error (SSE or RSS)
SSE <- sum((y - y_hat)^2)

# Display the results
c(SST = SST, SSReg = SSReg, SSE = SSE)


```



When looking at these 3 metrics individually, they kind of lack a significant practical interpretation. However, we will see now and we can use them collectively to show a new statistic that can measure the strength of regression model.

### Coefficient of Determination

The coefficient of determination is the fraction of the observed variation in y that can be accounted for or explained by the regression model.

$$R^2 = \frac {SSReg} {SST}$$

or

$$R^2 = 1- \frac {SSE} {SST}$$

**Question**- compute the $R^2$ for our example data in R


```{r}

# Calculate r-squared

R_squared <- SSReg / SST
R_squared


```



Interpretation- From our example dataset, our calculated $R^2$ = 0.65. Thus, we can conclude that 65% of the observed variation in stopping distance can be explained by the linear relationship with speed.




```{r}
#| echo: false
#2 * 2

#The `echo: false` option disables the printing of code (only output is displayed).

```
